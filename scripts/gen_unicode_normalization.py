#!/usr/bin/env python3
"""Generate Unicode normalization tables for the Scheme runtime."""
from __future__ import annotations

import unicodedata
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
OUTPUT = ROOT / "core" / "unicode_normalization_data.mbt"


def encode_char(cp: int) -> str:
    # MoonBit Char literal, e.g. '\u{1F600}'
    return fr"'\u{{{cp:X}}}'"


def format_char_array_let(
    name: str, tokens: list[str], items_per_line: int = 32
) -> str:
    if not tokens:
        return f"let {name} : ReadOnlyArray[Char] = []"

    lines: list[str] = []
    for i in range(0, len(tokens), items_per_line):
        chunk = tokens[i : i + items_per_line]
        lines.append("  " + ", ".join(chunk) + ",")
    body = "\n".join(lines)
    return f"let {name} : ReadOnlyArray[Char] = [\n{body}\n]"


canon_entries: list[tuple[int, list[int]]] = []
compat_entries: list[tuple[int, list[int]]] = []
combining_entries: list[tuple[int, int]] = []

for cp in range(0x110000):
    ch = chr(cp)
    decomp = unicodedata.decomposition(ch)
    if decomp:
        parts = decomp.split()
        if parts[0].startswith("<"):
            seq = [int(x, 16) for x in parts[1:]]
            if seq:
                compat_entries.append((cp, seq))
        else:
            seq = [int(x, 16) for x in parts]
            if seq:
                canon_entries.append((cp, seq))

    ccc = unicodedata.combining(ch)
    if ccc:
        combining_entries.append((cp, ccc))

canon_entries.sort(key=lambda x: x[0])
compat_entries.sort(key=lambda x: x[0])
combining_entries.sort(key=lambda x: x[0])

composition_entries: list[tuple[int, int, int]] = []
for cp, seq in canon_entries:
    if len(seq) == 2:
        a, b = seq
        if unicodedata.normalize("NFC", chr(a) + chr(b)) == chr(cp):
            composition_entries.append((a, b, cp))
composition_entries.sort(key=lambda x: (x[0], x[1], x[2]))

canon_tokens: list[str] = []
for cp, seq in canon_entries:
    canon_tokens.append(encode_char(cp))
    canon_tokens.append(encode_char(len(seq)))
    canon_tokens.extend(encode_char(item) for item in seq)

compat_tokens: list[str] = []
for cp, seq in compat_entries:
    compat_tokens.append(encode_char(cp))
    compat_tokens.append(encode_char(len(seq)))
    compat_tokens.extend(encode_char(item) for item in seq)

combining_tokens: list[str] = []
for cp, ccc in combining_entries:
    combining_tokens.append(encode_char(cp))
    combining_tokens.append(encode_char(ccc))

composition_tokens: list[str] = []
for a, b, c in composition_entries:
    composition_tokens.append(encode_char(a))
    composition_tokens.append(encode_char(b))
    composition_tokens.append(encode_char(c))

OUTPUT.write_text(
    "// Auto-generated by scripts/gen_unicode_normalization.py.\n"
    "// Source: Python's unicodedata tables.\n"
    "\n"
    f"const DECOMP_CANON_COUNT : Int = {len(canon_entries)}\n"
    f"{format_char_array_let('decomp_canon_data', canon_tokens)}\n"
    "\n"
    f"const DECOMP_COMPAT_COUNT : Int = {len(compat_entries)}\n"
    f"{format_char_array_let('decomp_compat_data', compat_tokens)}\n"
    "\n"
    f"const COMBINING_CLASS_COUNT : Int = {len(combining_entries)}\n"
    f"{format_char_array_let('combining_class_data_raw', combining_tokens)}\n"
    "\n"
    f"const COMPOSITION_COUNT : Int = {len(composition_entries)}\n"
    f"{format_char_array_let('composition_data_raw', composition_tokens)}\n",
    encoding="utf-8",
)
