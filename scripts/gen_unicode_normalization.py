#!/usr/bin/env python3
"""Generate Unicode normalization tables for the Scheme runtime."""
from __future__ import annotations

import unicodedata
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
OUTPUT = ROOT / "core" / "unicode_normalization_data.mbt"


def encode_cp(cp: int) -> str:
    return f"\\u{{{cp:X}}}"


def chunk_tokens(tokens: list[str], chunk_size: int) -> list[str]:
    return ["".join(tokens[i : i + chunk_size]) for i in range(0, len(tokens), chunk_size)]


def format_string_const(name: str, tokens: list[str], chunk_size: int = 256) -> str:
    if not tokens:
        return f"const {name} : String = \"\""
    chunks = chunk_tokens(tokens, chunk_size)
    if len(chunks) == 1:
        return f"const {name} : String =\n  $|{chunks[0]}"
    body = "\n  ".join(f"$|{chunk}" for chunk in chunks)
    return f"const {name} : String =\n  {body}"


canon_entries: list[tuple[int, list[int]]] = []
compat_entries: list[tuple[int, list[int]]] = []
combining_entries: list[tuple[int, int]] = []

for cp in range(0x110000):
    ch = chr(cp)
    decomp = unicodedata.decomposition(ch)
    if decomp:
        parts = decomp.split()
        if parts[0].startswith("<"):
            seq = [int(x, 16) for x in parts[1:]]
            if seq:
                compat_entries.append((cp, seq))
        else:
            seq = [int(x, 16) for x in parts]
            if seq:
                canon_entries.append((cp, seq))

    ccc = unicodedata.combining(ch)
    if ccc:
        combining_entries.append((cp, ccc))

canon_entries.sort(key=lambda x: x[0])
compat_entries.sort(key=lambda x: x[0])
combining_entries.sort(key=lambda x: x[0])

composition_entries: list[tuple[int, int, int]] = []
for cp, seq in canon_entries:
    if len(seq) == 2:
        a, b = seq
        if unicodedata.normalize("NFC", chr(a) + chr(b)) == chr(cp):
            composition_entries.append((a, b, cp))
composition_entries.sort(key=lambda x: (x[0], x[1], x[2]))

canon_tokens: list[str] = []
for cp, seq in canon_entries:
    canon_tokens.append(encode_cp(cp))
    canon_tokens.append(encode_cp(len(seq)))
    canon_tokens.extend(encode_cp(item) for item in seq)

compat_tokens: list[str] = []
for cp, seq in compat_entries:
    compat_tokens.append(encode_cp(cp))
    compat_tokens.append(encode_cp(len(seq)))
    compat_tokens.extend(encode_cp(item) for item in seq)

combining_tokens: list[str] = []
for cp, ccc in combining_entries:
    combining_tokens.append(encode_cp(cp))
    combining_tokens.append(encode_cp(ccc))

composition_tokens: list[str] = []
for a, b, c in composition_entries:
    composition_tokens.append(encode_cp(a))
    composition_tokens.append(encode_cp(b))
    composition_tokens.append(encode_cp(c))

OUTPUT.write_text(
    "// Auto-generated by scripts/gen_unicode_normalization.py.\n"
    "// Source: Python's unicodedata tables.\n"
    "\n"
    f"const DECOMP_CANON_COUNT : Int = {len(canon_entries)}\n"
    f"{format_string_const('DECOMP_CANON_DATA', canon_tokens)}\n"
    "\n"
    f"const DECOMP_COMPAT_COUNT : Int = {len(compat_entries)}\n"
    f"{format_string_const('DECOMP_COMPAT_DATA', compat_tokens)}\n"
    "\n"
    f"const COMBINING_CLASS_COUNT : Int = {len(combining_entries)}\n"
    f"{format_string_const('COMBINING_CLASS_DATA', combining_tokens)}\n"
    "\n"
    f"const COMPOSITION_COUNT : Int = {len(composition_entries)}\n"
    f"{format_string_const('COMPOSITION_DATA', composition_tokens)}\n",
    encoding="utf-8",
)
